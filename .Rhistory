select_best("rmse")
final_wf_tree <- preg_wf_tree_augustine %>%
finalize_workflow(bestTune_augustine) %>%
fit(data=augustine)
predictions_tree_augustine <- final_wf_tree %>%
predict(new_data = augustine)
predictions_tree_augustine
plot(predictions_tree_augustine)
augustine_data_frame <- as.data.frame(augustine)
ggplot(data=augustine_data_frame) +
geom_point(mapping= aes(x= Rating, y= (Average*Occupancy))) +
theme(aspect.ratio=1)
ggplot(data=augustine_data_frame) +
geom_point(mapping= aes(x= TimetoBeach, y= Average*Occupancy)) +
theme(aspect.ratio=1)
ggplot(data=augustine_data_frame) +
geom_point(mapping= aes(x= ClosetoStoreRating, y= High*Occupancy)) +
theme(aspect.ratio=1)
avPlots(augustine, terms= ~ Rooms + Baths + ProfessionalPhotography)
avPlots(augustine, Occupancy*Average= ~ Rooms + Baths + ProfessionalPhotography)
avPlots(augustine, Occupancy= ~ Rooms + Baths + ProfessionalPhotography)
avPlots(augustine_lm, Occupancy= ~ Rooms + Baths + ProfessionalPhotography)
avPlots(augustine_lm, Occupancy*Average= ~ Rooms + Baths + ProfessionalPhotography)
avPlots(augustine_lm, Average= ~ Rooms + Baths + ProfessionalPhotography)
#(a) Scatterplot
plot(augustine)
augustine_boxplot <- ggplot(augustine) +
geom_boxplot(mapping = aes( y= residuals)) +
theme(aspect.ratio = 1)
print(augustine_boxplot)
augustine_boxplot <- ggplot(augustine) +
geom_boxplot(mapping = aes( y= residuals)) +
theme(aspect.ratio = 1)
augustine$residuals <-  augustine_lm_avg$residuals
augustine_boxplot <- ggplot(augustine) +
geom_boxplot(mapping = aes( y= augustine_lm_avg$residuals)) +
theme(aspect.ratio = 1)
print(augustine_boxplot)
augustine_boxplot <- ggplot(augustine_lm_avg) +
geom_boxplot(mapping = aes(y=augustine_lm_avg$residuals)) +
theme(aspect.ratio = 1)
print(augustine_boxplot)
augustine_hist <- ggplot(data = augustine_lm_avg) +
geom_histogram(mapping = aes(x = residuals, y = ..density..),
) +
stat_function(fun = dnorm,
color = "red",
size = 2,
args = list(mean = mean(bodyfat$residuals),
sd = sd(bodyfat$residuals))) +
theme(aspect.ratio = 1)
augustine_hist <- ggplot(data = augustine_lm_avg) +
geom_histogram(mapping = aes(x = residuals, y = ..density..),
) +
stat_function(fun = dnorm,
color = "red",
size = 2,
args = list(mean = mean(augustine_lm_avg$residuals),
sd = sd(augustine_lm_avg$residuals))) +
theme(aspect.ratio = 1)
augustine_hist <- ggplot(data = augustine_lm_avg) +
geom_histogram(mapping = aes(x = residuals, y = ..density..),
) +
stat_function(fun = dnorm,
color = "red",
linewidth = 2,
args = list(mean = mean(augustine_lm_avg$residuals),
sd = sd(augustine_lm_avg$residuals))) +
theme(aspect.ratio = 1)
augustine_hist
augustine_hist <- ggplot(data = augustine_lm_avg) +
geom_histogram(mapping = aes(x = augustine_lm_avg$residuals, y = ..density..),
) +
stat_function(fun = dnorm,
color = "red",
linewidth = 2,
args = list(mean = mean(augustine_lm_avg$residuals),
sd = sd(augustine_lm_avg$residuals))) +
theme(aspect.ratio = 1)
augustine_hist
autoplot(augustine_lm_avg, which = 2, ncol = 1, nrow = 1) +
theme(aspect.ratio = 1)
autoplot(augustine_lm_avg, which = 2, ncol = 1, nrow = 1) +
theme(aspect.ratio = 1)
ggplot2::autoplot(augustine_lm_avg, which = 2, ncol = 1, nrow = 1) +
theme(aspect.ratio = 1)
autoplot(augustine, which = 2, ncol = 1, nrow = 1) +
theme(aspect.ratio = 1)
autoplot(augustine_lm_avg, which = 2, ncol = 1, nrow = 1) +
theme(aspect.ratio = 1)
ggplot(augustine_lm_avg, aes(sample = augustine_lm_avg) +
#(d) Normal Probability Plot
stat_qq() +
labs(title = "Normal Probability Plot")
ggplot(augustine_lm_avg  +
ggplot(augustine_lm_avg, aes(sample = augustine_lm_avg)  +
stat_qq() +
stat_qq_line() +
labs(title = "Normal Probability Plot"))
library(tidyverse)
library(ggfortify)
library(car)
library(corrplot)
library(gridExtra)
autoplot(bodyfat_lm, which = 2, ncol = 1, nrow = 1) +
theme(aspect.ratio = 1)
autoplot(augustine_lm_avg, which = 2, ncol = 1, nrow = 1) +
theme(aspect.ratio = 1)
# Residuals vs. Fitted Values Plot
resid_fitted_plot <- autoplot(augustine_lm_avg, which = 1, ncol = 1, nrow = 1)
print(resid_fitted_plot)
# Cook's Distance
augustine$cooksd <- cooks.distance(augustine_lm_avg)
ggplot(data = bodyfat) +
geom_point(mapping = aes(x = as.numeric(rownames(bodyfat)),
y = cooksd)) +
ylab("Cook's Distance") +
xlab("Observation Number") +
geom_hline(mapping = aes(yintercept = 4 / length(cooksd)),
color = "red",
linetype = "dashed") +
theme(aspect.ratio = 1)
# Cook's Distance
augustine_lm_avg$cooksd <- cooks.distance(augustine_lm_avg)
ggplot(data = augustine_lm_avg) +
geom_point(mapping = aes(x = as.numeric(rownames(bodyfat)),
y = cooksd)) +
ylab("Cook's Distance") +
xlab("Observation Number") +
geom_hline(mapping = aes(yintercept = 4 / length(cooksd)),
color = "red",
linetype = "dashed") +
theme(aspect.ratio = 1)
ggplot(data = augustine_lm_avg) +
geom_point(mapping = aes(x = as.numeric(rownames(augustine_lm_avg)),
y = cooksd)) +
ylab("Cook's Distance") +
xlab("Observation Number") +
geom_hline(mapping = aes(yintercept = 4 / length(cooksd)),
color = "red",
linetype = "dashed") +
theme(aspect.ratio = 1)
# Cook's Distance
augustine_lm_avg$cooksd <- cooks.distance(augustine_lm_avg)
ggplot(data = augustine_lm_avg) +
geom_point(mapping = aes(x = as.numeric(rownames(augustine_lm_avg)),
y = cooksd)) +
ylab("Cook's Distance") +
xlab("Observation Number") +
geom_hline(mapping = aes(yintercept = 4 / length(cooksd)),
color = "red",
linetype = "dashed") +
theme(aspect.ratio = 1)
# Cook's Distance
augustine_lm_avg$cooksd <- cooks.distance(augustine_lm_avg)
ggplot(data = augustine_lm_avg) +
geom_point(mapping = aes(x = as.numeric(rownames(augustine_lm_avg)),
y = abs(dffits))) +
ylab("Absolute Value of DFFITS for Y") +
xlab("Observation Number") +
geom_hline(mapping = aes(yintercept = 2 * sqrt(length(bodyfat_lm$coefficients) /
length(dffits))),
color = "red",
linetype = "dashed") +
theme(aspect.ratio = 1)
# DFFITS
augustine_lm_avg$dffits <- dffits(augustine_lm_avg)
ggplot(data = augustine_lm_avg) +
geom_point(mapping = aes(x = as.numeric(rownames(augustine_lm_avg)),
y = abs(dffits))) +
ylab("Absolute Value of DFFITS for Y") +
xlab("Observation Number") +
geom_hline(mapping = aes(yintercept = 2 * sqrt(length(bodyfat_lm$coefficients) /
length(dffits))),
color = "red",
linetype = "dashed") +
theme(aspect.ratio = 1)
vif(bodyfat_lm)
vif(augustine_lm_avg)
summary(augustine_lm_avg)
summary(augustine_lm_low)
summary(augustine_lm_high)
summary(augustine_lm_Occupancy)
summary(augustine_lm_avg_only)
#Best Model from this Data:
augustine_lm_avg <- lm(Occupancy*Average ~ Rooms + Baths + ProfessionalPhotography + FreeParking, data=augustine)
summary(augustine_lm_best)
#Best Model from this Data:
augustine_lm_best <- lm(Occupancy*Average ~ Rooms + Baths + ProfessionalPhotography + FreeParking, data=augustine)
summary(augustine_lm_best)
predictions_tree_augustine
collect_metrics(CV_results_tree_augustine) %>%
filter(.metric=="rmse") %>%
ggplot(data=., aes(x=penalty, y=mean, color=factor(mixture))) +
geom_line()
collect_metrics(CV_results_tree_augustine) %>%
filter(.metric=="rmse") %>%
ggplot(data=., aes(x=penalty, y=mean, color=factor(mixture))) +
geom_line()
avPlots(augustine_lm_avg, Average= ~ Rooms + Baths + ProfessionalPhotography)
autoplot(augustine_lm_avg, which = 2, ncol = 1, nrow = 1) +
theme(aspect.ratio = 1)
collect_metrics(CV_results_tree_augustine) %>%
filter(.metric=="rmse") #%>%
rsq(predictions_tree_augustine)
rsq(truth = augustine$Occupancy, estimate = predictions_tree_augustine)
collect_metrics(CV_results_tree_augustine) %>%
filter(.metric=="rmse") #%>%
final_wf_tree
my_recipe_augustine <- recipe(Occupancy*Average ~ ., data=augustine)
my_recipe_augustine <- recipe(Average ~ ., data=augustine)
prepped_recipe_augustine <- prep(my_recipe_augustine)
bake(prepped_recipe_augustine, new_data=augustine)
my_mod_tree_augustine <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n=tune()) %>% #Type of model
set_engine("rpart") %>% # Engine = What R function to use
set_mode("regression")
## Create a workflow with model & recipe
preg_wf_tree_augustine <- workflow() %>%
add_recipe(my_recipe_augustine) %>%
add_model(my_mod_tree_augustine)
## Set up grid of tuning values
tuning_grid_tree_augustine <- grid_regular(tree_depth(),
cost_complexity(),
min_n(),
levels = 5)
## Set up K-fold CV
folds <- vfold_cv(augustine, v =5, repeats=1)
## Find best tuning parameters
CV_results_tree_augustine <- preg_wf_tree_augustine %>%
tune_grid(resamples=folds,
grid=tuning_grid_tree_augustine,
metrics=metric_set(rmse, mae, rsq))
bestTune_augustine <- CV_results_tree_augustine %>%
select_best("rmse")
final_wf_tree <- preg_wf_tree_augustine %>%
finalize_workflow(bestTune_augustine) %>%
fit(data=augustine)
predictions_tree_augustine <- final_wf_tree %>%
predict(new_data = augustine)
predictions_tree_augustine
collect_metrics(CV_results_tree_augustine) %>%
filter(.metric=="rmse") #%>%
final_wf_tree
augustine_tree <- augustine %>%
mutate(OccAvg = Occupancy * Average) %>%
select(-High, -Low)
my_recipe_augustine <- recipe(OccAvg ~ ., data=augustine)
augustine_tree <- augustine %>%
mutate(OccAvg = Occupancy * Average) %>%
select(-High, -Low)
my_recipe_augustine <- recipe(OccAvg ~ ., data=augustine)
augustine_tree
my_recipe_augustine <- recipe(OccAvg ~ ., data=augustine_tree)
prepped_recipe_augustine <- prep(my_recipe_augustine)
bake(prepped_recipe_augustine, new_data=augustine_tree)
my_mod_tree_augustine <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n=tune()) %>% #Type of model
set_engine("rpart") %>% # Engine = What R function to use
set_mode("regression")
## Create a workflow with model & recipe
preg_wf_tree_augustine <- workflow() %>%
add_recipe(my_recipe_augustine) %>%
add_model(my_mod_tree_augustine)
## Set up grid of tuning values
tuning_grid_tree_augustine <- grid_regular(tree_depth(),
cost_complexity(),
min_n(),
levels = 5)
## Set up K-fold CV
folds <- vfold_cv(augustine_tree, v =5, repeats=1)
## Find best tuning parameters
CV_results_tree_augustine <- preg_wf_tree_augustine %>%
tune_grid(resamples=folds,
grid=tuning_grid_tree_augustine,
metrics=metric_set(rmse, mae, rsq))
bestTune_augustine <- CV_results_tree_augustine %>%
select_best("rmse")
final_wf_tree <- preg_wf_tree_augustine %>%
finalize_workflow(bestTune_augustine) %>%
fit(data=augustine_tree)
predictions_tree_augustine <- final_wf_tree %>%
predict(new_data = augustine_tree)
predictions_tree_augustine
collect_metrics(CV_results_tree_augustine) %>%
filter(.metric=="rmse") #%>%
final_wf_tree
augustine_tree <- augustine %>%
mutate(OccAvg = Occupancy * Average) %>%
select(-High, -Low, -Occupancy, -Average)
augustine_tree
my_recipe_augustine <- recipe(OccAvg ~ ., data=augustine_tree)
prepped_recipe_augustine <- prep(my_recipe_augustine)
bake(prepped_recipe_augustine, new_data=augustine_tree)
my_mod_tree_augustine <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n=tune()) %>% #Type of model
set_engine("rpart") %>% # Engine = What R function to use
set_mode("regression")
## Create a workflow with model & recipe
preg_wf_tree_augustine <- workflow() %>%
add_recipe(my_recipe_augustine) %>%
add_model(my_mod_tree_augustine)
## Set up grid of tuning values
tuning_grid_tree_augustine <- grid_regular(tree_depth(),
cost_complexity(),
min_n(),
levels = 5)
## Set up K-fold CV
folds <- vfold_cv(augustine_tree, v =5, repeats=1)
## Find best tuning parameters
CV_results_tree_augustine <- preg_wf_tree_augustine %>%
tune_grid(resamples=folds,
grid=tuning_grid_tree_augustine,
metrics=metric_set(rmse, mae, rsq))
bestTune_augustine <- CV_results_tree_augustine %>%
select_best("rmse")
final_wf_tree <- preg_wf_tree_augustine %>%
finalize_workflow(bestTune_augustine) %>%
fit(data=augustine_tree)
predictions_tree_augustine <- final_wf_tree %>%
predict(new_data = augustine_tree)
predictions_tree_augustine
collect_metrics(CV_results_tree_augustine) %>%
filter(.metric=="rmse") #%>%
final_wf_tree
RMSE = sqrt(nanmean((OccAvg_predicted-OccAvg).^2))
1 - mse / Var(OccAvg)
mse(predictions_tree_augustine)
performance_mse(predictions_tree_augustine)
summary(final_wf_tree)
final_wf_tree
performance::performance_mse(predictions_tree_augustine)
library(performance)
performance::mse(predictions_tree_augustine)
performance::mse(final_wf_tree)
CV_results_tree_augustine
bestTune_augustine
predictions_tree_augustine
(predictions_tree_augustine - augustine_tree)
predictions_tree_augustine
augustine_tree
(predictions_tree_augustine - augustine_tree$OccAvg)
(predictions_tree_augustine - augustine_tree$OccAvg)
(predictions_tree_augustine - augustine_tree$OccAvg)^2/75
sum(predictions_tree_augustine - augustine_tree$OccAvg)
sum(predictions_tree_augustine - augustine_tree$OccAvg)^2
sum((predictions_tree_augustine - augustine_tree$OccAvg)^2)
sum((predictions_tree_augustine - augustine_tree$OccAvg)^2)/75
LR_R = RSQUARE(augustine_tree$OccAvg,predictions_tree_augustine)
RSQUARE = function(y_actual,y_predict){
cor(y_actual,y_predict)^2
}
LR_R = RSQUARE(augustine_tree$OccAvg,predictions_tree_augustine)
LR_R
RSqaured_Augustine
RSQUARE = function(y_actual,y_predict){
cor(y_actual,y_predict)^2
}
RSquared_Augustine = RSQUARE(augustine_tree$OccAvg,predictions_tree_augustine)
RSquared_Augustine
knitr::opts_chunk$set(echo = TRUE)
###Loading Packages###
library(tidyverse)
library(corrplot)
library(bestglm)
library(car)
library(vroom)
library(tidymodels)
view(augustine)
###Importing Data and Creating a "Recipe"###
augustine <- vroom("StAugustine.csv") %>%
select(-PL, -PH)
view(augustine)
mutate(3mAvg = (DaysBooked3Months/92)*Average %>%
mutate(ThreeMAvg = (DaysBooked3Months/92)*Average %>%
augustine <- augustine %>%
###Importing Data and Creating a "Recipe"###
augustine <- vroom("StAugustine.csv") %>%
select(-PL, -PH)
###Importing Data and Creating a "Recipe"###
augustine <- vroom("StAugustine.csv") %>%
select(-PL, -PH)
augustine <- augustine %>%
###EDA###
augustine <- augustine %>%
mutate(BookedAvg = (DaysBookedinNext30/30)*Average) %>%
mutate(ThreeMAvg = (DaysBooked3Months/92)*Average %>%
select(-KidFriendly, -ClimateControl, -FastWifi, -KitchenEssentials, -'W&D')
ggplot(data = augustine) +
mutate(ThreeAvg = (DaysBooked3Months/92)*Average %>%
setwd("C:/Users/brook/Downloads/STAT348/Amazon")
library(tidyverse)
library(ggplot2)
library(vroom)
library(tidymodels)
library(embed)
library(ranger)
library(discrim)
library(naivebayes)
library(kknn)
#Read in Data
train <- vroom('./train.csv/train.csv') %>%
mutate(ACTION = as.factor(ACTION))
test <- vroom('./test.csv/test.csv')
#SMOTE - Imbalanced Model
library(tidymodels)
library(themis) # for smote
my_recipe_smote <- recipe(ACTION ~., data=train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
step_lencode_mixed(all_nominal_predictors(), outcome= vars(ACTION)) %>%
step_normalize(all_predictors()) %>% #Everything numeric for SMOTE so encode it here
step_smote(all_outcomes(), neighbors=5)
# apply the recipe to your data
prepped_recipe_smote <- prep(my_recipe_smote)
baked <- bake(prep, new_data = train)
baked <- bake(prep, new_data = train)
baked <- bake(prepped_recipe_smote, new_data = train)
knn_model_smote <- nearest_neighbor(neighbors=tune()) %>%
set_mode("classification") %>%
set_engine("kknn")
knn_wf_pca <- workflow() %>%
add_recipe(my_recipe_) %>%
add_model(knn_model_pca)
knn_wf_pca <- workflow() %>%
add_recipe(my_recipe_) %>%
add_model(knn_model_smote)
knn_wf_pca <- workflow() %>%
add_recipe(my_recipe_smote) %>%
add_model(knn_model_smote)
## Fit or Tune Model HERE
tuning_grid_knn_pca <- grid_regular(neighbors(),
levels = 5) ## L^2 total tuning possibilities
## Set up K-fold CV
folds <- vfold_cv(train, v = 5, repeats=1)
knn_wf_smote <- workflow() %>%
add_recipe(my_recipe_smote) %>%
add_model(knn_model_smote)
## Fit or Tune Model HERE
tuning_grid_knn_pca <- grid_regular(neighbors(),
levels = 5) ## L^2 total tuning possibilities
## Set up K-fold CV
folds <- vfold_cv(train, v = 5, repeats=1)
CV_results_knn_smote <- knn_wf_smote %>%
tune_grid(resamples=folds,
grid=tuning_grid_knn_pca,
metrics=metric_set(roc_auc))
## Find best tuning parameters
bestTune_knn_pca <- CV_results_knn_smote %>%
select_best("roc_auc")
final_wf_knn_pca <- knn_wf_smote %>%
finalize_workflow(bestTune_knn_pca) %>%
fit(data=train)
predictions_knn_pca <- final_wf_knn_pca %>%
predict(test, type = "prob")
predictions_knn_pca <- predictions_knn_pca %>%
bind_cols(., test) %>%
select(id, .pred_1) %>%
rename(action = .pred_1)
vroom_write(x= predictions_knn_pca, file="predictions_knn_smote.csv", delim=",")
#Smote Linear SVM
## SVM models
svmPoly <- svm_poly(degree=tune(), cost=tune()) %>% # set or tune
set_mode("classification") %>%
set_engine("kernlab")
svmRadial <- svm_rbf(rbf_sigma=tune(), cost=tune()) %>% # set or tune
set_mode("classification") %>%
set_engine("kernlab")
svmLinear <- svm_linear(cost=tune()) %>% # set or tune
set_mode("classification") %>%
set_engine("kernlab")
wf_svm <- workflow() %>%
add_recipe(my_recipe_smote) %>%
add_model(svmLinear)
## Fit or Tune Model HERE
tuning_grid_svm <- grid_regular(rbf_sigma(),
cost(),
levels = 5) ## L^2 total tuning possibilities
## Set up K-fold CV
folds <- vfold_cv(train, v = 5, repeats=1)
CV_results_svm <- wf_svm %>%
tune_grid(resamples=folds,
grid=tuning_grid_svm,
metrics=metric_set(roc_auc))
## Fit or Tune Model HERE
tuning_grid_svm <- grid_regular(cost(),
levels = 5) ## L^2 total tuning possibilities
## Set up K-fold CV
folds <- vfold_cv(train, v = 5, repeats=1)
CV_results_svm <- wf_svm %>%
tune_grid(resamples=folds,
grid=tuning_grid_svm,
metrics=metric_set(roc_auc))
CV_results_svm <- wf_svm %>%
tune_grid(resamples=folds,
grid=tuning_grid_svm,
metrics=metric_set(roc_auc))
## Find best tuning parameters
bestTune_svm <- CV_results_svm %>%
select_best("roc_auc")
final_wf_svm <- wf_svm %>%
finalize_workflow(bestTune_svm) %>%
fit(data=train)
predictions_svm <- final_wf_svm %>%
predict(test, type = "prob")
predictions_svm <- predictions_svm %>%
bind_cols(., test) %>%
select(id, .pred_1) %>%
rename(action = .pred_1)
vroom_write(x= predictions_svm, file="predictions_svm_linear_smote.csv", delim=",")
